---
title: "Social cues modulate the representations underlying cross-situational learning"
output: kmr::els_manuscript
bibliography: "soc-xsit.bib"
csl: apa6.csl
document-params: "authoryear, review"
journal: "Cognitive Psychology"

author-information:
    # Group authors per affiliation:
    - \author[km]{\corref{cor}Kyle MacDonald}
    - \cortext[cor]{Corresponding author}
    - \ead{kyle.macdonald@stanford.edu}
    - \author[dy]{Daniel Yurovsky}
    - \author[mcf]{Michael C. Frank}
    - \address{Department of Psychology, Stanford University, United States}

abstract:
    "Because children hear language in environments that contain many things to talk about, learning the meaning of even the simplest word requires making inferences under uncertainty. A cross-situational statistical learner can aggregate across naming events to form stable word-referent mappings, but this approach neglects an important source of information that can reduce referential uncertainty: social cues from speakers (e.g., eye gaze). In four large-scale experiments with adults, we tested the effects of varying referential uncertainty in cross-situational word learning using social cues. Social cues shifted learners away from tracking multiple hypotheses and towards storing only a single hypothesis (Experiments 1 and 2). In addition, learners were sensitive to graded changes in the strength of a social cue, and when it became less reliable, they were more likely to store multiple hypotheses (Experiment 3). Finally, learners stored fewer word-referent mappings in the presence of a social cue even when visual inspection time was equivalent to naming events without a social cue present (Experiment 4). Taken together, our data suggest that the representations underlying cross-situational word learning are quite flexible: In conditions of greater uncertainty, learners store a broader range of information."

keywords: 
    "statistical learning, social cues, word learning, language acquisition"
---

```{r global_options, cache=F, include = F}
rm(list=ls())
knitr::opts_chunk$set(out.width = "100%", fig.path='figs/',
                      fig.align = "center", fig.pos = "tb", 
                      echo=F, warning=F, cache=T,
                      message=F, sanitize = T)
```

```{r libraries, cache=F}
# devtools::install_github("kemacdonald/kmr") # uncomment to install 
source("../analysis/useful.R"); library(kmr); library(langcog); 
library(dplyr); library(magrittr); library(modelr); library(tidyr)
library(lme4); library(xtable)

# citeproc does not differentiate between parathentical and in-text citations
# so it doesn't know when we need to use an ampersand (parathentical cite) vs. "and" (in-text cite)
# messing things up for apa formatting
# we can use the knitcitations package to write the inline citations as r code

# devtools::install_github("cboettig/knitcitations") # uncomment to install 
library(knitcitations)
bib <- read.bibtex("soc-xsit.bib")
options("citation_format" = "text")
```

```{r data}
df_expt1 <- read.csv("../data/3_final-processed/soc-xsit-expt1-finalData.csv", stringsAsFactors = F)
df_expt2 <- read.csv("../data/3_final-processed/soc-xsit-expt2-finalData.csv", stringsAsFactors = F)
df_expt3 <- read.csv("../data/3_final-processed/soc-xsit-expt3-finalData.csv", stringsAsFactors = F)
df_expt4 <- read.csv("../data/3_final-processed/soc-xsit-expt4-finalData.csv", stringsAsFactors = F)
```

```{r chance_functions}
# Logit function for passing chance performance offsets to glms
logit <- function(x) {log(x/(1-x))}

# Fits logistic regressions to estimate coefficients and significance values 
# for individual factors
test.chance <- function(data,groups,formula,row=1) {
  chance.tests <- data %>%
    group_by_(.dots = groups) %>%
    do(chance.lm = summary(glm(formula, offset=logit(1/numPicN),
                               family="binomial", data = .)))
  
  chance.tests$betas<- sapply(chance.tests$chance.lm,
                              function(x) {x$coefficients[row,1]})
  chance.tests$zs <- sapply(chance.tests$chance.lm,
                            function(x) {x$coefficients[row,3]})
  chance.tests$ps <- sapply(chance.tests$chance.lm,
                            function(x) {x$coefficients[row,4]})
  
  return(select(chance.tests, -chance.lm))
}

## get stars for significance testing
getstars <- function(x) {
  if (x > .1) {return("")}
  if (x < .001) {return("***")}
  if (x < .01) {return("**")}
  if (x < .05) {return("*")}
  return(".")}
```

\newpage

# Introduction

Learning the meaning of a new word should be hard. Consider that even concrete nouns are often used in complex contexts with multiple possible referents, which in turn have many conceptually natural properties that a speaker could talk about. This ambiguity creates the potential for an (in principle) unlimited amount of referential uncertainty in the learning task.^[This problem is a simplified version of Quine's \textit{indeterminacy of reference} [@quine19600]: That there are many possible meanings for a word ("Gavigai") that include the referent ("Rabbit") in their extension, e.g., "white," "rabbit," "dinner." Quine's broader philosophical point was that different meanings ("rabbit" and "undetached rabbit parts") could actually be extensionally identical and thus impossible to tease apart.] Remarkably, word learning proceeds despite this uncertainty, with estimates of adult vocabularies ranging between 50,000 to 100,000 distinct words [@bloom2002children]. How do learners infer and retain such a large variety of word meanings from data with this kind of ambiguity?

Statistical learning theories offer a solution to this problem by aggregating cross-situational statistics across labeling events to identify underlying word meanings [@yu2007rapid; @siskind1996computational]. Recent experimental work has shown that both adults and young infants can use word-object co-occurrence statistics to learn words from individually ambiguous naming events [@smith2008infants; @vouloumanos2008fine]. For example, `r knitcitations::citet(bib[["smith2008infants"]])` taught 12-month-olds three novel words simply by repeating consistent novel word-object pairings across 10 ambiguous exposure trials. Moreover, computational models suggest that cross-situational learning can scale up to learn adult-sized lexicons, even under conditions of considerable referential uncertainty [@smith2011cross].

Although all cross-situational learning models agree that the input is the co-occurrence between words and objects and the output is stable word-object mappings, they disagree about how closely learners approximate the input distribution (for review, see Smith, Suanda, & Yu 2014). One approach has been to model learning as a process of updating connection strengths between multiple word-object links [@mcmurray2012word], while other approaches have argued that learners store only a single word-object hypothesis [@trueswell2013propose]. Recent experimental and modeling work, `r knitcitations::citet(bib[["yurovsky2014algorithmic"]])` suggests an integrative explanation: learners allocate a fixed amount of attention to a single hypothesis, and distribute the rest evenly among the remaining alternatives. As the set of alternatives grows, the amount of attention allocated to each object approaches zero.

In addition to the debate about representation, researchers have disagreed about how to characterize the ambiguity of the input to cross-situational learning mechanisms. One way to quantify the uncertainty in a naming event is to show adults clips of caregiver-child interactions and measure their accuracy at guessing the meaning of an intended referent (Human Simulation Paradigm: HSP [Gillette, Gleitman, Gleitman, and Lederer, 1999]). Using the HSP, `r knitcitations::citet(bib[["medina2011words"]])` found that approximately 90% of learning episodes were ambiguous (< 33% accuracy) and only 7% were relatively unambiguous (> 50% accuracy). In contrast, `r knitcitations::citet(bib[["yurovsky2013statistical"]])` found a higher proprotion of clear naming events, with approximately 30% being unambiguous (> 90% accuracy). Consistent with this finding, `r knitcitations::citet(bib[["cartmill2013quality"]])` showed that the proportion of unambiguous naming episodes varies across parent-child dyads, with some parents rarely providing highly informative contexts and others’ doing so relatively more often.^[The differences in the estimates of referential uncertainty in these studies could be driven by the different sampling procedures used to select naming events for the HSP. `r knitcitations::citet(bib[["yurovsky2013statistical"]])` sampled utterances for which the parent labeled a co-present object, whereas `r knitcitations::citet(bib[["medina2011words"]])` randomly sampled any utterances containing concrete nouns. Regardless of these differences, the key point here is that variability in referential uncertainty across naming events exists and thus could alter the representations underlying cross-situational learning.]

Thus, representations in cross-situational word learning can appear distributional or discrete, and the input to statistical learning mechanisms can vary along a continuum from low to high ambiguity. These results raise an interesting question: could learners be sensitive to the ambiguity of the input and use this information to alter the representations they store in memory? In the current line of work, we investigated how the presence of referential cues in the social context might alter the ambiguity of the input to statistical word learning mechanisms.

Social-pragmatic theories of language acquisition emphasize the importance of social cues for word learning [@bloom2002children; @clark2009first; @hollich2000breaking]. Experimental work has shown that even children as young as 16 months prefer to map novel words to objects that are the target of a speaker’s gaze and not their own [@baldwin1993infants]. In an analysis of naturalistic parent-child labeling events, `r knitcitations::citet(bib[["yu2012embodied"]])` found that young learners tended to retain labels that were accompanied by clear referential cues, which served to make a single object dominant in the visual field. And correlational studies have demonstrated strong links between early intention-reading skills (e.g., gaze following) and later vocabulary growth [@brooks2005development; @brooks2008infant; @carpenter1998social]. Moreover, studies outside the domain of language acquisition have shown that the presence of social cues: (a) produce better spatial learning of audiovisual events [@wu2011infants], (b) boost recognition of a cued object [@cleveland2007joint], and (c) lead to preferential encoding of an object’s featural information [@yoon2008communication]. Together, the evidence suggests that social cues could alter the representations stored during cross-situational word learning by modulating how people allocate attention to the relevant statistics in the input.

The goal of our current investigation is to ask whether the presence of a valid social cue -- a speaker's gaze -- can change the representations underlying cross-situational word learning. We used a modified version of `r knitcitations::citet(bib[["yurovsky2014algorithmic"]])`'s paradigm to provide a direct measure of memory for alternative word-object links during cross-situational learning. In Experiment 1, we manipulated the presence of a referential cue at different levels of attention and memory demands. At all levels of difficulty, learners tracked a strong single hypothesis but were less likely to track multiple word-object links when a social cue was present. In Experiment 2, we replicated the findings from Experiment 1 using a more ecologically valid social cue. In Experiment 3, we moved to a parametric manipulation of referential uncertainty by varying the reliability of the speaker's gaze. Learners were sensitive to graded changes in reliability and retained more word-object links as uncertainty in the input increased. Finally, in Experiment 4, we equated the length of the initial naming events with and without the referential cue. Learners stored less information in the presence of gaze even when they had visually inspected the objects for the same amount of time. In sum, our data suggest that cross-situational word learners are quite flexible, storing representations with different levels of fidelity depending on the amount of ambiguity present during learning.    

# Experiment 1 

We set out to test the effect of a referential cue on the representations underlying cross-situational word learning. We used a version of `r knitcitations::citet(bib[["yurovsky2014algorithmic"]])`'s paradigm where we manipulated the ambiguity of the learning context by including a gaze cue from a schematic, female interlocutor. Participants saw a series of ambiguous exposure trials where they heard one novel word that was either paired with a gaze cue or not and selected the object they thought went with each word. In subsequent test trials, participants heard the novel word again, this time paired with a new set of novel objects. One of the objects in this set was either the participant's initial guess (Same test trials) or one of the objects was *not* their initial guess (Switch test trials). Performance on Switch trials provided a direct measure of whether referential cues influenced the number of alternative word-object links that learners stored in memory. If learners performed worse on Switch trials after an exposure trial with gaze, this would suggest that they stored fewer additional objects from the initial learning context.

## Method

### Participants

```{r e1 participants}
# experiment 1
nsubs_expt1 <- df_expt1 %>%
  group_by(condition, intervalNum, numPicN) %>%
  summarise(n_subs = n_distinct(subid)) 

final_n_expt1 <- sum(nsubs_expt1$n_subs)

# get exlcuded participants for duplicates (we kept the first HIT)
n_excluded_e1_dups <- df_expt1 %>% 
  filter(duplicate == T) %>% 
  distinct(subid) %>% 
  summarise(n_excluded = n_distinct(subid))

# get exlcuded participants for failing to follow gaze
n_excluded_e1_acc_exp <- df_expt1 %>% 
  filter(mean_acc_exp < 0.25) %>% 
  summarise(n_excluded = n_distinct(subid))
```

We posted a set of Human Intelligence Tasks (HITs) to Amazon Mechanical Turk. Only participants with US IP addresses and a task approval rate above 95% were allowed to participate, and each HIT paid 30 cents. 50-100 HITs were posted for each of the 32 between-subjects conditions. Data were excluded if participants completed the task more than once or if participants did not respond correctly on familiar object trials (`r n_excluded_e1_dups` HITs). The final sample consisted of `r final_n_expt1 - n_excluded_e1_dups` participants. 


```{r stimuli, fig.pos = "tb", out.width="80%",  fig.cap = "Screenshots of exposure and test trials from Experiment 1 (schematic gaze cue) and Experiments 2, 3 \\& 4 (video gaze cue). Participants saw exposure trials with or without a gaze cue depending on condition assignment. All participants saw both types of test trials: Same and Switch. On Same trials, the object that participants chose during exposure appeared with a new novel object. On Switch trials the object that participants did not choose appeared with a new novel object."}

grid::grid.raster(png::readPNG("figs/stimuli_new.png"))
```

### Stimuli

Figure 1 shows screenshots taken from Experiment 1. Visual stimuli were black and white pictures of familiar and novel objects taken from `r knitcitations::citet(bib[["kanwisher1997locus"]])`. Auditory stimuli were recordings of familiar and novel words by an AT&T Natural Voices \texttrademark (voice: Crystal) speech synthesizer. Novel words were 1-3 syllable pseudowords that obeyed all rules of English phonotactics. A schematic drawing of a human speaker was chosen for ease of manipulating the direction of gaze, the referential cue of interest in this study. All experiments can be viewed and downloaded at the project page: https://kemacdonald.github.io/soc_xsit/.

### Design and Procedure

Participants saw a total of 16 trials: eight exposure trials and eight test trials. On each trial, they heard one novel word, saw a set of novel objects, and were asked to guess which object went with the word. Before seeing exposure and test trials, participants completed four practice trials with familiar words and objects. These trials familiarized participants to the task and allowed us to exclude participants who were unlikely to perform the task as directed, either because of inattention or because their computer audio was turned off. 

After the practice trials, participants were told that they would now hear novel words and see novel objects and that their task was to select the referent that "goes with each word." Over the course of the experiment, participants heard eight novel words two times, with one exposure trial and one test trial for each word. Four of the test trials were *Same* trials in which the object that participants selected on the exposure trial was shown with a set of new novel objects. The other four test trials were *Switch* trials in which one of the objects was chosen at random from the set of objects that the participant did not select on exposure. 

Participants were randomly assigned to one of the 32 between-subjects conditions (4 Referents X 4 Intervals X 2 Gaze conditions). Participants either saw 2, 4, 6, or 8 referents on the screen and test trials occurred at different intervals after exposure trials: either 0, 1, 3, or 7 trials from the initial exposure to a word. For example, in the 0-interval condition, the test trial for that word would occur immediately following the exposure trial, but in the 3-interval condition, participants would see three additional exposure trials for other novel words before seeing the test trial for the initial word. The interval conditions modulated the time delay and the number of intervening trials between learning and test, and the number of referents conditions modulated the attention demands present during learning. 

Participants were assigned to either the Gaze or No-Gaze condition. In the Gaze condition, gaze was directed towards one of the objects on exposure trials; in the No-Gaze condition, gaze was always directed straight ahead (see Figure 1 for examples). At test, gaze was always directed straight ahead. To show participants that their response had been recorded, a red box appeared around the selected object for one second. This box always appeared around the selected object, even if participants' selections were incorrect.

## Results and Discussion

### Analysis plan

The structure of our analysis plan is parallel across all four experiments. First, we examined accuracy and response time on exposure trials to provide evidence that learners were (a) sensitive to our experimental manipulation and (b) altered their allocation of attention in response to the presence of a social cue. Accuracy on exposure trials was defined as selecting the referent that was the target of gaze in the Gaze condition. (Note that there was no "correct" behavior for exposure trials in the No-Gaze condition.) Next, we examined accuracy on test trials to test whether learners' memory for alternative word-object links changed depending on the ambiguity of the learning context. Accuracy on test trials (both Same and Switch) was defined as selecting the referent that was present during the exposure trial for that word.

The key behavioral prediction of our hypothesis is that the presence of gaze would result in reduced memory for multiple word-object links, operationalized as a decrease in accuracy on Switch test trials after seeing exposure trials with a gaze cue. To quantify participants' behavior, we used mixed-effects regression models with the maximal random effects structure justified by our experimental design: by-subject intercepts and slopes for each trial type [@barr2013random]. We limited all models to include only two-way interactions because the critical test of our hypothesis was the interaction between gaze condition and trial type, and we did not have theoretical predictions for any possible three-way or four-way interactions. All models were fit using the lme4 package in R [@bates2013lme4], and all of our data and our processing/analysis code can be viewed in the version control repository for this paper at https://github.com/kemacdonald/soc_xsit.

```{r expt1-plot, out.width = "90%", fig.cap = "Experiment 1 results. The top row shows average inspection times on exposure trials for all experimental conditions as a function of the number of trials that occurred between exposure and test. Each panel represents a different number of referents, and line color represents the Gaze and No-Gaze conditions. The bottom row shows accuracy on test trials for all conditions as a function of the number of intervening trials. The horizontal dashed lines represent chance performance for each number of referents, and the type of line (solid vs. dashed) represents the different test trial types (Same vs. Switch). Error bars indicate 95\\% confidence intervals computed by non-parametric bootstrap."}

grid::grid.raster(png::readPNG("figs/expt1_new.png"))
```

### Exposure trials

```{r expt1 get inspection times exposure trials}
df_expt1 %<>% 
  filter(trial_category == "exposure", include_good_rt_exposure == "include") %>% 
  select(subid, itemNum, inspection_time_exposure = rt) %>% 
  mutate(inspection_time_exposure_sec = inspection_time_exposure / 1000) %>% 
  left_join(df_expt1, by = c("subid", "itemNum")) 
```

```{r expt1 chance tests exposure trials}
e1.chance.tests.exposure <- test.chance(data = filter(df_expt1, trial_category == "exposure", 
                                                      include_good_rt_exposure == "include", 
                                                      include_expo == "include",
                                                      condition == "Social"),
                                        groups = c("numPicN","intervalNum"),
                                        formula = as.formula("correct_exposure ~ 1")) %>%
  mutate(stars = getstars(ps))
```

```{r rt exposure expt1 lmer}
m1_rt_expt1 <- lmer(log2(inspection_time_exposure_sec) ~ 
                      (condition + log2(intervalNum + 1) + log2(numPicN))^2 + (1|subid), 
                    data=filter(df_expt1, 
                                trial_category == "exposure", 
                                include_good_rt_exposure == "include",
                                include_expo == "include"))


# use normal distribution to approximate p-value
e1_rt_coefs <- data.frame(coef(summary(m1_rt_expt1)))
e1_rt_coefs$p.z <- 2 * (1 - pnorm(abs(e1_rt_coefs$t.value)))

# exctract betas and p-vals
beta_referents <- round(e1_rt_coefs[4,1], 2)
p_referents <- ifelse(round(e1_rt_coefs[4,4], 3) == 0, "p < .001", round(e1_rt_coefs[4,4], 3))

beta_gaze_interval <- round(e1_rt_coefs[5,1], 2)
p_gaze_interval <- ifelse(round(e1_rt_coefs[5,4], 3) == 0, "p < .001", round(e1_rt_coefs[5,4], 3))

beta_gaze_referents <- round(e1_rt_coefs[6,1], 2)
p_gaze_referents <- ifelse(round(e1_rt_coefs[6,4], 3) == 0, "p < .001", round(e1_rt_coefs[6,4], 3))
```

```{r expt1_exposure_means}
ms_expo_expt1 <- df_expt1 %>% 
  filter(trial_category == "exposure", 
         include_good_rt_exposure == "include", 
         condition == "Social", include_expo == "include") %>% 
  group_by(numPicN, intervalNum) %>% 
  summarise(prop_correct = round(mean(correct_exposure), 2)) 

ms_expo_expt1_collapsed <- df_expt1 %>% 
  filter(trial_category == "exposure", 
         include_good_rt_exposure == "include", 
         condition == "Social", include_expo == "include") %>% 
  summarise(prop_correct = mean(correct_exposure))

m_follow_gaze_expt1 <- round(ms_expo_expt1_collapsed$prop_correct, 2)
```

To ensure that our referential cue manipulation was effective, we compared participants' accuracies on exposure trials in the Gaze condition to a model of random behavior defined as a Binomial distribution with a probability of success $\frac{1}{Num Referents}$. Correct performance was defined as selecting the object that was the target of the speaker's gaze. Following `r knitcitations::citet(bib[["yurovsky2014algorithmic"]])`, we fit logistic regressions for each gaze, referent, and interval combination specified as \texttt{Gaze Target $\sim$ 1 + offset(logit(1/Referents))}. The offset encoded the chance probability of success given the number of referents, and the coefficient for the intercept term shows on a log-odds scale how much more likely participants were to select the gaze target than would be expected if participants were selecting randomly. In all conditions, participants used gaze to select referents on exposure trials more often than expected by chance (smallest $\beta$ = `r round(sort(e1.chance.tests.exposure$betas)[1], 2)`, z = `r round(sort(e1.chance.tests.exposure$zs)[1], 2)`, $p$ < .001). However, there was variability across conditions in the mean proportion of gaze cue (overall $M$ = `r m_follow_gaze_expt1`, range: `r min(ms_expo_expt1$prop_correct)`--`r max(ms_expo_expt1$prop_correct)`). 

We were also interested in differences in participants' response times across the experimental conditions. Since these trials were self-paced, participants could choose how much time to spend inspecting the referents on the screen, thus providing an index of participants' attention. To quantify the effects of gaze, interval, and number of referents, we fit a linear mixed-effects model that predicted participants' inspection times as follows: \texttt{Log(Inspection time) $\sim$ Gaze * Log(Interval) + Gaze * Log(Referents) + (1 | subject)}. We found a significant main effect of the number of referents ($\beta$ = `r beta_referents`, `r p_referents`) with longer inspection times as the number of referents increased, a significant interaction between gaze condition and the number of referents ($\beta$ = `r beta_gaze_referents`, `r p_gaze_referents`) with longer inspection times in the No-Gaze condition, especially as the number of referents increased, and a significant interaction between gaze condition and interval ($\beta$ = `r beta_gaze_interval`, $p$ = `r p_gaze_interval`) with slower inspection times in the No-Gaze condition, especially as the number of intervening trials increased (see the top row of Figure 2). Shorter inspection times on exposure trials with gaze provides evidence that the presence of a referential cue focused participants' attention on a single referent and away from alternative word-object links. 

### Test trials

```{r expt1_chance}
e1.chance.tests <- test.chance(data = filter(df_expt1, trial_category == "test", 
                                             include_good_rt_test == "include",
                                             include_expo == "include" | condition == "No-Social",
                                             correct_exposure == T | condition == "No-Social"),
                               groups = c("trialType","numPicN","intervalNum"),
                               formula = as.formula("correct ~ 1")) %>%
  mutate(stars = getstars(ps))
```

```{r expt1 glmer}
m1_2way_acc_expt1 <- glmer(correct ~ (trialType + condition + log2(intervalNum + 1) +
                                        log2(numPicN))^2 + (trialType | subid), 
                           offset = logit(1/numPicN), 
                           control=glmerControl(optimizer="bobyqa"),
                           family=binomial, 
                           nAGQ=0,
                           data=filter(df_expt1, trial_category == "test", 
                                       include_good_rt_test == "include",
                                       include_expo == "include" | condition == "No-Social",
                                       correct_exposure == T | condition == "No-Social"))


# get coefs and convert p.vals to .001 if below .001
m1_2way_coefs <- as.data.frame(coef(summary(m1_2way_acc_expt1)))
m1_2way_coefs %<>% 
  mutate(predictor = row.names(.),
         p.vals = ifelse(`Pr(>|z|)` < .001, "< .001", round(`Pr(>|z|)`, 3)),
         Estimate = round(Estimate, 2)) %>% 
  select(predictor, everything())

# two-way main effects betas
beta_trialtype_acc <- m1_2way_coefs$Estimate[2]
beta_gaze_acc <- m1_2way_coefs$Estimate[3]
beta_interval_acc <- m1_2way_coefs$Estimate[4]
beta_referents_acc <- m1_2way_coefs$Estimate[5]

# two-way main effects p.vals
p_trialtype_acc <- m1_2way_coefs$p.vals[2]
p_gaze_acc <- m1_2way_coefs$p.vals[3]
p_interval_acc <- m1_2way_coefs$p.vals[4]
p_referents_acc <- m1_2way_coefs$p.vals[5]

# two-way interactions
beta_gaze_trialtype_int <- m1_2way_coefs$Estimate[6]
beta_trialtype_interval_int <- m1_2way_coefs$Estimate[7]
beta_trialtype_referents_int <- m1_2way_coefs$Estimate[8]
beta_gaze_referents_int <- m1_2way_coefs$Estimate[10]

# two-way interactions p.vals
p_gaze_trialtype_int <- m1_2way_coefs$p.vals[6]
p_trialtype_interval_int <- m1_2way_coefs$p.vals[7]
p_trialtype_referents_int <- m1_2way_coefs$p.vals[8]
p_gaze_referents_int <- m1_2way_coefs$p.vals[10]
```

Next, we explored participants' accuracy in identifying the referent for each word in all conditions for both kinds of test trials (see the bottom row of Figure 2). We first compared the distribution of correct responses made by each participant to the distribution expected if participants were selecting randomly defined as a Binomial distribution with a probability of success $\frac{1}{Num Referents}$. Correct performance is defined as selecting the object that was present on the exposure trial for that word. We fit the same logistic regressions as we did for exposure trials: \texttt{Correct $\sim$ 1 + offset(logit(1/Referents))}. In 31 out of the 32 conditions for both Same and Switch trials, participants chose the correct object more often than would be expected by chance (smallest $\beta$ = `r round(sort(e1.chance.tests$betas)[2], 2)`, $z$ = `r round(sort(e1.chance.tests$zs)[2], 2)`, $p$ = `r round(sort(e1.chance.tests$ps, decreasing = T)[2], digits = 2)`). On Switch trials in the 8-referent, 3-interval condition, participants' responses were not significantly different from chance ($\beta$ = `r round(sort(e1.chance.tests$betas)[1], 2)`, z = `r round(sort(e1.chance.tests$zs)[1], 2)`, $p$ = `r round(sort(e1.chance.tests$ps, decreasing = T)[1], digits = 2)`). Participants' success on Switch trials replicates the findings from `r knitcitations::citet(bib[["yurovsky2014algorithmic"]])` and provides direct evidence that learners encode more than a single hypothesis in ambiguous word learning situations even under high attentional and memory demands and in the presence of a referential cue.

```{r e1 table, echo = F, results = 'asis'}
e1.tab <- as.data.frame(summary(m1_2way_acc_expt1)$coef)

e1.tab$Predictor <- c("Intercept",
                      "Switch Trial",
                      "Gaze Condition",
                      "Log(Interval)",
                      "Log(Referents)",
                      "Switch Trial*Gaze Condition",
                      "Switch Trial*Log(Interval)",
                      "Switch Trial*Log(Referent)",
                      "Gaze Condition*Log(Interval)",
                      "Gaze Condition*Log(Referent)",
                      "Log(Interval)*Log(Referent)")

rownames(e1.tab) <- NULL
e1.tab <- e1.tab[,c(5,1:4)]
names(e1.tab)[4:5] <- c("$z$ value","$p$ value")

e1.tab %<>% 
  mutate(
    stars = ifelse(`$p$ value` > .1, "", 
                   ifelse(`$p$ value` < .001, "***",
                          ifelse(`$p$ value` < .01, "**",
                                 ifelse(`$p$ value` < .05, "*",
                                        ifelse(`$p$ value` < .1, ".", "Error"))))),
    `$p$ value` = ifelse(`$p$ value` > .1, round(`$p$ value`, 2), 
                         ifelse(`$p$ value` < .001, "$<$ .001",
                                ifelse(`$p$ value` < .01, round(`$p$ value`, 2),
                                       ifelse(`$p$ value` < .05, round(`$p$ value`, 2),
                                              ifelse(`$p$ value` < .1, round(`$p$ value`, 2), 
                                                     "Error")))))
  )

names(e1.tab)[6] <- c("")

print(xtable(e1.tab,
             align = c("l","l","r","r","r","r","l"),
             label = "tab:exp1_reg",
             caption = "Predictor estimates with standard errors and significance information for a logistic mixed-effects model predicting word learning in Experiment 1."),
      include.rownames=FALSE,hline.after=c(0,nrow(e1.tab)),
      sanitize.text.function=function(x){x},
      caption.placement = 'bottom', 
      table.placement = "tb",
      comment = F)
```

```{r e1 3way model}
# now fit the 3way model
m1_3way_acc_expt1 <- glmer(correct ~ (trialType + condition + log2(intervalNum + 1) + log2(numPicN))^3 +
                             (trialType | subid), 
                           offset = logit(1/numPicN),
                           control=glmerControl(optimizer="bobyqa"),
                           family=binomial, 
                           nAGQ=0, 
                           data=filter(df_expt1, trial_category == "test", 
                                       include_good_rt_test == "include", 
                                       include_expo == "include" | condition == "No-Social",
                                       correct_exposure == T | condition == "No-Social"))

# key two-way interaction between trial type and gaze
beta_gaze_trialtype_int_2way <- round(summary(m1_3way_acc_expt1)$coef[6], 2)
p_gaze_trialtype_int_2way <- round(summary(m1_3way_acc_expt1)$coef[6,4], 3)

# three-way interaction
beta_gaze_trialtype_3way_int <- round(summary(m1_3way_acc_expt1)$coef[12], 2)
p_gaze_trialtype_3way_int <- round(summary(m1_3way_acc_expt1)$coef[12,4], 3)
```

To quantify the effects of gaze, interval, and number of referents on the probability of a correct response, we fit the following mixed-effects logistic regression model to a filtered dataset where we removed participants who did not reliably select the object that was the target of gaze on exposure trials: ^[We did not predict that there would be a subset of participants who would not follow the gaze cue, thus this filtering criteria was developed posthoc. However, we think that the filter is theoretically motivated because we would only expect to see an effect of gaze if participants actually used the gaze cue. The filter removed `r n_excluded_e1_acc_exp` participants (`r round((n_excluded_e1_acc_exp / final_n_expt1)*100, 1)`% of the sample). The key inferences from the data do not depend on this filtering criteria.] \texttt{Correct $\sim$ Trial Type * Gaze + Trial Type * Log(Interval) + Trial Type * \\ Log(Referents) + offset(logit($^1/_{Referents}$)) + (TrialType | subject)}. 

We coded interval and number of referents as continuous predictors and transformed these variables to the log scale. We limited the model to include only two-way interactions because the critical test of our hypothesis is the interaction between gaze condition and trial type, and we did not have any theoretical predictions for possible three-way interactions. \footnote{If we allowed for three-way interactions in the model, there was a marginally significant interaction between gaze condition, trial type, and interval ($\beta = `r beta_gaze_trialtype_3way_int`$, $p$ = `r p_gaze_trialtype_3way_int`). The two-way interaction between gaze condition and trial type remained significant in this more complex model ($\beta = `r beta_gaze_trialtype_int_2way`$, $p$ = `r p_gaze_trialtype_int_2way`).} 

Table 1 shows the output of the logistic regression. We found significant main effects of the number of referents ($\beta = `r beta_referents_acc`$, $p$ < .001) and interval ($\beta = `r beta_interval_acc`$, $p$ < .001), such that as each of these factors increased, accuracy on test trials decreased. We also found a significant main effect of trial type ($\beta = `r beta_trialtype_acc`$, $p$ < .001), with worse performance on Switch trials. There were significant interactions between trial type and interval ($\beta = `r beta_trialtype_interval_int`$, $p$ < .001), trial type and referents ($\beta = `r beta_trialtype_referents_int`$, $p$ < .001), and gaze condition and referents ($\beta = `r beta_gaze_referents_int`$, $p$ < .05). These interactions can be interpreted as meaning: (a) the interval between exposure and test affected Same trials more than Switch trials, (b) the number of referents affected Switch trials more than Same trials, and (c) participants performed slightly better at the higher number of referents in the Gaze condition. The interactions between gaze condition and referents and between referents and interval were not significant. Importantly, we found the predicted interaction between trial type and gaze condition ($\beta = `r beta_gaze_trialtype_int`$, $p$ < .001), with participants in the Gaze condition performing worse on Switch trials. This interaction provides direct evidence that the presence of a referential cue selectively reduces participants' memory for alternative word-object links. 

```{r e1 inspection model}
m1_2way_acc_expt1_inspect <- glmer(correct ~ (trialType + condition + 
                                                log2(intervalNum + 1) +
                                                log2(numPicN) + 
                                                log2(inspection_time_exposure_sec))^2 + 
                                     (trialType | subid), 
                                   offset = logit(1/numPicN), 
                                   control=glmerControl(optimizer="bobyqa"),
                                   family=binomial, 
                                   nAGQ=0,
                                   data=filter(df_expt1, trial_category == "test", 
                                               include_good_rt_exposure == "include",
                                               include_good_rt_test == "include",
                                               include_expo == "include" | condition == "No-Social",
                                               correct_exposure == T | condition == "No-Social"))

# get the beta and pvalue for inspection time main effect
beta_inspect_expt1 <- round(summary(m1_2way_acc_expt1_inspect)$coef[6], 2)
p_inspect_expt1 <- round(summary(m1_2way_acc_expt1_inspect)$coef[6,4], 2)

# get the beta and pvalue for inspection time interaction with gaze condition
beta_gaze_inspect_e1 <- round(summary(m1_2way_acc_expt1_inspect)$coef[13], 2)
p_gaze_inspect_e1 <- round(summary(m1_2way_acc_expt1_inspect)$coef[13, 4], 2)

# get beta and p.val for trial type and gaze condition interation
beta_gaze_ttype_inspect_e1 <- round(summary(m1_2way_acc_expt1_inspect)$coef[7], 2)
p_gaze_ttype_inspect_e1 <- "p < .001"
```

We were also interested in how inspection times on exposure trials would affect participants' accuracy at test. So we fit an additional model where participants' inspection times were added as a predictor. We found a significant interaction between inspection time and gaze condition ($\beta = `r beta_gaze_inspect_e1`$, $p$ = `r p_gaze_inspect_e1`), such that longer inspection times provided a larger boost to accuracy in the No-Gaze condition. This interaction suggests that the presence of a referential cue modulated the relationship between attention on exposure trials and memory at test. Importantly, the key test of our hypothesis, the interaction between gaze condition and trial type, remained significant in this alternative version of the model ($\beta$ = `r beta_gaze_ttype_inspect_e1`, $p$ = `r p_gaze_ttype_inspect_e1`). 

Taken together, the inspection time and accuracy analyses provide evidence that the presence of a referential cue modulated learners' attention during learning, and in turn made them less likely to track multiple word-object links. We did not see strong evidence that reduced tracking of alternatives resulted in an increase in performance on Same trials. This finding suggests that the limitations on Same trials may be different than those regulating the distribution of attention on Switch trials since the presence of a referential cue selectively reduced learners tracking of alternatives but apparently did not cause learners to form a stronger memory of their single candidate hypothesis. 

There was relatively large variation in performance across conditions in group-level accuracy scores and in participants' tendency to *use* the referential cue on exposure trials. Moreover, we found a subset of participants who did not reliably use the gaze cue at all, potentially reducing the effect of gaze on cross-situational learning in this experiment. It is possible that the effect of gaze was reduced because the referential cue that we used -- a static schematic drawing of a speaker -- was relatively weak compared to the cues present in real-world learning environments. Thus we do not yet know how learners' memory for alternatives during cross-situational learning would change in the presence of a stronger and more ecologically valid referential cue. We designed Experiment 2 to address this question. 

# Experiment 2

In Experiment 2, we set out to replicate the findings from Experiment 1 using a more ecologically valid stimulus set. We replaced the static, schematic drawing with a video of a female actress. While these stimuli were still far from actual learning contexts, they included a real person who provided both a gaze cue and a head turn towards the target object. To reduce the across-conditions variability that we found in Experiment 1, we introduced a within-subjects design where each participant saw both Gaze and No-Gaze exposure trials in a blocked design. We selected a subset of the conditions from Experiment 1 and tested only the 4-referent display with 0 and 3 intervening trials as between-subjects manipulations. Our goals were to replicate the reduction in learners' tracking of alternative word-object links in the presence of a referential cue and to test whether increasing the ecological validity of the cue would result in a boost to the strength of learners' recall of their candidate hypothesis.  

## Method

### Participants

```{r e2 participants}
# experiment 2
nsubs_expt2 <- df_expt2 %>% 
  group_by(condition, interval) %>%
  summarise(n_subs = n_distinct(subid)) %>% 
  mutate(n_excluded = 100 - n_subs)

n_hits_e2 <- 400
n_excluded_e2 <- sum(nsubs_expt2$n_excluded)
```

Participant recruitment and inclusion/exclusion criteria were identical to those of Experiment 1. 100 HITs were posted for each condition (1 Referent X 2 Intervals X 2 Gaze conditions) for total of `r n_hits_e2` paid HITs (`r n_excluded_e2` HITs excluded). 

```{r e2 filter}
df_expt2_analysis <-df_expt2 %>% 
  filter(include == TRUE, 
         mean_acc_exp > 0.25, 
         include_good_rt == "include")

df_expt2_analysis %<>% 
  rename(inspection_time_exposure = rt_exposure) %>% 
  mutate(inspection_time_exposure_sec = inspection_time_exposure / 1000)

df_expt2_analysis %<>% 
  mutate(include_good_rt_exposure = ifelse(log(inspection_time_exposure_sec) >
                                             mean(log(inspection_time_exposure_sec)) + 2 *
                                             sd(log(inspection_time_exposure_sec)) |
                                    log(inspection_time_exposure_sec) <
                                      mean(log(inspection_time_exposure_sec)) - 2 *
                                      sd(log(inspection_time_exposure_sec)),
                                  "exclude", "include")) %>% 
  filter(include_good_rt_exposure == "include")
```

### Stimuli

Audio and picture stimuli were identical to Experiment 1. The referential cue in the Gaze condition was a video (see Figure 1). On each exposure trial, the actress looked out at the participant with a neutral expression, smiled, and then turned to look at one of the four images on the screen. She maintained her gaze for 3 seconds before returning to the center. On test trials, she looked straight ahead for the duration of the trial. 

## Design and Procedure

Procedures were identical to those of Experiment 1. The major design change was a within-subjects manipulation of the gaze cue where each participant saw exposure trials with and without gaze. The experiment consisted of 32 trials split into 2 blocks of 16 trials. Each block consisted of 8 exposure trials and 8 test trials (4 Same trials and 4 Switch trials) and contained only Gaze or No-gaze exposure trials. The order of block was counterbalanced across participants. 

## Results and Discussion

We followed the same analysis plan as in Experiment 1. We first analyzed inspection times and accuracy on exposure trials and then analyzed accuracy on test trials.

### Exposure trials

```{r expt2_chance_exposure}
e2.chance.tests.exposure <- test.chance(data = filter(df_expt2_analysis, 
                                                      trial_category == "exposure", 
                                                      include_good_rt == "include", 
                                                      condition_trial == "social"),
                                        groups = c("numPicN","intervalNum"),
                                        formula = as.formula("correct_exposure ~ 1")) %>%
  mutate(stars = getstars(ps))
```

```{r expt2 lmer rt}
m1_rt_expt2 <- lmer(inspection_time_exposure_sec ~ 
                      condition_trial * log2(intervalNum + 1) + (1|subid), 
                    data=filter(df_expt2_analysis, trial_category == "exposure"))

beta_gaze_rt_expt2 <- round(summary(m1_rt_expt2)$coef[2], 2)
beta_interval_rt_expt2 <- round(summary(m1_rt_expt2)$coef[3], 2)
```

```{r e2 mean gaze follow}
gf_e1_4pic <- ms_expo_expt1 %>% 
  filter(numPicN == 4) %>% 
  group_by(numPicN) %>% 
  summarise(m = round(mean(prop_correct), 2))

ms_expo_expt2 <- df_expt2_analysis %>% 
  filter(trial_category == "exposure", 
         include_good_rt == "include", 
         condition_trial == "social") %>% 
  group_by(numPicN) %>% 
  summarise(m = round(mean(correct_exposure), 2))
```

Similar to Experiment 1, participants' responses on exposure trials differed from those expected by chance (smallest $\beta$ = `r round(sort(e2.chance.tests.exposure$betas)[1], 2)`, z = `r round(sort(e2.chance.tests.exposure$zs)[1], 2)`, $p$ < .001), suggesting that gaze was effective in directing participants' attention. Participants in Experiment 2 were more consistent in their use of gaze with the video stimuli compared to the schematic stimuli used in Experiment 1 ($M_{Exp1}$ = `r gf_e1_4pic$m`, $M_{Exp2}$ = `r ms_expo_expt2$m`$), suggesting that using a real person increased participants' willingness to follow the gaze cue.

We replicated the findings from Experiment 1. Inspection times were shorter in the Gaze ($\beta$ = `r beta_gaze_rt_expt2`, $p$ < .001) and the 3-interval condition ($\beta$ = `r beta_interval_rt_expt2`, $p$ < .001). The interaction between gaze and interval was not significant, meaning that gaze had the same effect on participants' inspection times at both intervals (see Panel A of Figure 3).

```{r expt2-plot, out.width="80%", fig.cap = "Experiment 2 results. Panel A shows inspection times on exposure trials with and without gaze. Panel B shows accuracy on Same and Switch test trials. All plotting conventions are the same as in Figure 2. Error bars indicate 95\\% confidence intervals computed by non-parametric bootstrap."}
grid::grid.raster(png::readPNG("figs/expt2_new.png"))
```

### Test trials

```{r expt2_chance}
e2.chance.tests <- test.chance(data = filter(df_expt2_analysis, trial_category == "test"),
                               groups = c("trialType","numPicN","intervalNum"),
                               formula = as.formula("correct ~ 1")) %>%
  mutate(stars = getstars(ps))
```

```{r expt 2 acc glmer}
m1_acc_expt2 <- glmer(correct ~ (trialType + condition_trial + log2(intervalNum + 1))^2 +
                        (trialType | subid), 
                      nAGQ=1,
                      glmerControl(optimizer = "bobyqa"),
                      family=binomial,
                      data = filter(df_expt2_analysis, trial_category == "test"))

beta_trialtype_expt2 <- round(summary(m1_acc_expt2)$coef[2], 2)
beta_interval_expt2 <- round(summary(m1_acc_expt2)$coef[4], 2)
beta_trialtype_gaze_expt2 <- round(summary(m1_acc_expt2)$coef[5], 2)
beta_trialtype_interval_expt2 <- round(summary(m1_acc_expt2)$coef[6], 2)
beta_interval_gaze_expt2 <- round(summary(m1_acc_expt2)$coef[7], 2)
beta_interval_gaze_expt2_p <- round(summary(m1_acc_expt2)$coef[7,4], 3)
```

```{r expt2-table, echo = F, results = 'asis'}
e2.tab <- as.data.frame(summary(m1_acc_expt2)$coef)

e2.tab$Predictor <- c("Intercept",
                      "Switch Trial",
                      "Gaze Condition",
                      "Log(Interval)",
                      "Switch Trial*Gaze Condition",
                      "Switch Trial*Log(Interval)",
                      "Gaze Condition*Log(Interval)")

rownames(e2.tab) <- NULL
e2.tab <- e2.tab[,c(5,1:4)]
names(e2.tab)[4:5] <- c("$z$ value","$p$ value")

e2.tab %<>% 
  mutate(
    stars = ifelse(`$p$ value` > .1, "", 
                   ifelse(`$p$ value` < .001, "***",
                          ifelse(`$p$ value` < .01, "**",
                                 ifelse(`$p$ value` < .05, "*",
                                        ifelse(`$p$ value` < .1, ".", "Error"))))),
    `$p$ value` = ifelse(`$p$ value` > .1, round(`$p$ value`, 2), 
                         ifelse(`$p$ value` < .001, "$<$ .001",
                                ifelse(`$p$ value` < .01, round(`$p$ value`, 2),
                                       ifelse(`$p$ value` < .05, round(`$p$ value`, 2),
                                              ifelse(`$p$ value` < .1, round(`$p$ value`, 2), 
                                                     "Error")))))
  )


names(e2.tab)[6] <- c("")

print(xtable(e2.tab,
             align = c("l","l","r","r","r","r","l"),
             label = "tab:exp2_reg",
             caption = "Predictor estimates with standard errors and significance information for a logistic mixed-effects model predicting word learning in Experiment 2."),
      include.rownames=FALSE,hline.after=c(0,nrow(e2.tab)),
      sanitize.text.function=function(x){x},
      caption.placement = 'bottom', 
      table.placement = "tb",
      comment = F)
```

Across all conditions for both trial types participants selected the correct referent at rates greater than chance (smallest $\beta$ = `r round(sort(e2.chance.tests$betas)[1], 2)`, z = `r round(sort(e2.chance.tests$zs)[1], 2)`, $p$ < .001). We replicated the critical finding from Experiment 1: after seeing exposure trials with gaze, participants performed worse on Switch trials, meaning they stored fewer word-object links ($\beta =  `r beta_trialtype_gaze_expt2`$, $p$ < .001).\footnote{As in Experiment 1, we fit this model to a filtered dataset removing participants who did not reliably use the gaze cue.} Participants were also less accurate as the interval between exposure and test increased ($\beta$ = `r beta_interval_expt2`, $p$ < .001) and on the Switch trials overall ($\beta = `r beta_trialtype_expt2`$, $p$ < .001). 

In addition, there was a significant interaction between trial type and interval ($\beta = `r beta_trialtype_interval_expt2`$, $p$ < .001), with worse performance on Switch trials in the 3-interval condition. The interaction between gaze condition and interval was marginally significant ($\beta = `r beta_interval_gaze_expt2`$, $p$ = `r beta_interval_gaze_expt2_p`), such that participants in the gaze condition were less affected by the increase in interval. Similar to Experiment 1, we did not see evidence of a boost to performance on Same trials in the gaze condition. 

```{r e2 inspect model}
m2_inspect_e2 <- glmer(correct ~ (condition_trial + trialType + log2(intervalNum+1) +
                                    log2(inspection_time_exposure_sec))^2 +
                         (trialType | subid), 
                       nAGQ=0,
                       glmerControl(optimizer = "bobyqa"),
                       family=binomial,
                       data = filter(df_expt2_analysis, trial_category == "test"))

e2_m_inspect_coefs <- broom::tidy(m2_inspect_e2) %>% 
  mutate_at(.cols = c("estimate", "std.error", "statistic", "p.value"), 
            .funs = round, digits = 3) %>% 
  mutate(estimate = round(estimate, 2))
```

Next, we added inspection times on exposure trials as a predictor of accuracy at test. We found a marginally significant main effect of inspection time ($\beta$ = `r e2_m_inspect_coefs$estimate[5]`, $p$ = `r e2_m_inspect_coefs$p.value[5]`) with higher accuracy as inspection time increased. Similar to Experiment 1, the interaction between gaze and trial type remained significant even when inspection time was added to the model ($\beta$ = `r e2_m_inspect_coefs$estimate[6]`, $p$ < .001). 

The results of Experiment 2 provide converging evidence for our hypothesis that the presence of a referential cue reliably focuses learners' attention away from alternative word-object links and shifts them towards single hypothesis tracking. Moving to the video stimulus led to higher rates of selecting the target of gaze on exposure trials, but did not result in a boost to performance on Same trials. The selective effect of gaze on Switch trials provides additional evidence that the fidelity of participants' single hypothesis was unaffected by the presence of a referential cue in our paradigm. 

Thus far we have shown that people store different amounts of information in response to a categorical manipulation of referential uncertainty. In both Experiments 1 and 2, the learning context was either entirely ambiguous (No-Gaze) or entirely unambiguous (Gaze). But not all real-world learning contexts fall at the extremes of this continuum. Could learners be sensitive to more subtle changes in the quality of learning contexts? In our next experiment, we tested a prediction of our account: whether learners would store more word-object links in response to graded changes in referential uncertainty during learning.

# Experiment 3

In Experiment 3, we explored whether learners would allocate attention and memory flexibly in response to *graded* changes in the referential uncertainty that was present during learning. To test this hypothesis, we moved beyond a categorical manipulation of the presence/absence of gaze, and we parametrically varied the reliability of the referential cue. We manipulated cue reliability by adding a block of familiarization trials where we varied the proportion of Same and Switch trials. If participants saw more Switch trials, this provided direct evidence that the speaker's gaze was a less reliable cue to reference because the gaze target on exposure trials would not appear at test. This design was inspired by a growing body of experimental work showing that even young children are sensitive to the prior reliability of speakers and will use this information to decide whom to learn novel words from (e.g., Koenig, Clement, & Harris, 2004). 

## Method

### Participants

```{r e3 participants}
# experiment 3
e3_nhits <- 100
nsubs_expt3 <- df_expt3 %>%
  filter(experiment == "replication") %>% 
  group_by(prop_cond_clean) %>%
  summarise(n_subs = n_distinct(subid)) %>% 
  mutate(n_excluded = e3_nhits - n_subs)

final_n_expt3 <- sum(nsubs_expt3$n_subs)
nsubs_excluded_expt3 <- sum(nsubs_expt3$n_excluded)
```

Participant recruitment and inclusion/exclusion criteria were identical to those of Experiment 1 and 2 (`r nsubs_excluded_expt3` HITs excluded). 100 HITs were posted for each reliability level (0%, 25%, 50%, 75%, and 100%) for total of 500 paid HITs.  

### Design and Procedure

Procedures were identical to those of Experiments 1 and 2. We modified the design of our cross-situational learning paradigm to include a block of 16 familiarization trials (8 exposure trials and 8 test trials) at the beginning of the experiment. These trials served to establish the reliability of the speaker's gaze. To establish reliability, we varied the proportion of Same/Switch trials that occurred during the familiarization block. Recall that on Switch trials the gaze target did not show up at test, which provided evidence that the speaker's gaze was not a reliable cue to reference. Reliability was a between-subjects manipulation such that participants either saw 8, 6, 4, 2, or 0 Switch trials during familiarization, which created the 0%, 25%, 50%, 75%, and 100% reliability conditions. After the familiarization block, participants completed another block of 16 trials (8 exposure trials and 8 test trials). Since we were no longer testing the effect of the presence or absence of a referential cue, all exposure trials throughout the experiment included a gaze cue. Finally, at the end of the task, we asked participants to assess the reliability of the speaker on a continuous scale from "completely unreliable" to "completely reliable." 

## Results and Discussion

### Exposure trials

```{r e3 chance exposure test block}
e3.chance.tests.exposure <- test.chance(data = filter(df_expt3, trial_category == "exposure", 
                                                      block == "test", experiment == "replication", 
                                                      include_good_rt == "include"),
                                        groups = c("prop_cond_clean"),
                                        formula = as.formula("correct ~ 1")) %>%
  mutate(stars = getstars(ps))
```

```{r expt3 glmer accuracy exposure test block}
m1_expo_expt3 <- glmer(correct ~ reliability * rel_subj  + (1 | subid), 
                       nAGQ = 1,
                       control = glmerControl(optimizer = "bobyqa"),
                       family = binomial,
                       data = filter(df_expt3, trial_category == "exposure", 
                                     block == "test", experiment == "replication", 
                                     include_good_rt == "include"))

# coefs
beta_rel_exposure_expt3 <- round(summary(m1_expo_expt3)$coef[2], 2)
p_rel_exposure_expt3 <- round(summary(m1_expo_expt3)$coef[2, 4], 2)

beta_rel_subj_exposure_expt3 <- round(summary(m1_expo_expt3)$coef[3], 2)
beta_rel_subj_exposure_interaction_expt3 <- round(summary(m1_expo_expt3)$coef[4], 2)

# marginal p
beta_rel_subj_exposure_interaction_expt3_p <- round(summary(m1_expo_expt3)$coef[4,4], 3)
```

```{r e3 gaze following means}
ms_expt3 <- df_expt3 %>% 
  filter(trial_category == "test", block == "test", 
         include_good_rt == "include", experiment == "replication") %>% 
  group_by(reliability) %>%
  summarise(accuracy = mean(correct_exposure, na.rm=T)) %>% 
  mutate(accuracy = round(accuracy, 2))

m0 <- ms_expt3$accuracy[1]
m25 <- ms_expt3$accuracy[2]
m50 <- ms_expt3$accuracy[3]
m75 <- ms_expt3$accuracy[4]
m100 <- ms_expt3$accuracy[5]
```

Participants reliably chose the referent that was the target of gaze at rates greater than chance (smallest $\beta$ = `r round(sort(e3.chance.tests.exposure$betas)[1], 2)`, z = `r round(sort(e2.chance.tests.exposure$zs)[1], 2)`, $p$ < .001). We fit a mixed effects logistic regression model predicting the probability of selecting the gaze target as follows: \texttt{Correct-Exposure $\sim$ Reliability Condition * Subjective Reliability + (1 | subject)}. We found an effect of reliability condition ($\beta$ = `r beta_rel_exposure_expt3`, $p$ = `r p_rel_exposure_expt3`) such that when the gaze cue was more reliable, participants were more likely to use it ($M_{0\%}$ = `r m0`, $M_{25\%}$ = `r m25`, $M_{50\%}$ = `r m50`, $M_{75\%}$ = `r m75`, $M_{100\%}$ = `r m100`). We also found an effect of subjective reliability ($\beta$ = `r beta_rel_subj_exposure_expt3`, $p$ < .001) such that when participants thought the gaze cue was reliable, they were more likely to use it. The interaction between reliability condition and subjective reliability assessments was marginally significant ($\beta$ = `r beta_rel_subj_exposure_interaction_expt3`, $p$= `r beta_rel_subj_exposure_interaction_expt3_p`). This analysis provides evidence that participants were sensitive to the reliability manipulation both in how often they used the gaze cue and in how they rated the reliability of the speaker at the end of the task.

### Test trials

```{r e3-plot, out.width = "90%", fig.cap = "Primary analyses of test trial performance in Experiment 3. Panel A shows performance as a function of reliability condition. Panel B shows performance as a function of reliability condition and whether participants chose to follow gaze on exposure trials. The horizontal dashed lines represent chance performance, and error bars indicate 95\\% confidence intervals computed by non-parametric bootstrap."}
grid::grid.raster(png::readPNG("figs/expt3_main_plot.png"))
```

```{r expt3_chance}
e3.chance.tests <- test.chance(data = filter(df_expt3, trial_category == "test",
                                             block == "test",
                                             experiment == "replication", 
                                             include_good_rt == "include"),
                               groups = c("trialType","prop_cond_clean"),
                               formula = as.formula("correct ~ 1")) %>%
  mutate(stars = getstars(ps))
```

Next, we tested whether the reliability manipulation altered the strength of participants' memory for alternative word-object links. Across all conditions, participants selected the correct referent at rates greater than chance (smallest $\beta$ = `r round(sort(e3.chance.tests$betas)[1], 2)`, z = `r round(sort(e3.chance.tests$zs)[1], 2)`, $p$ < .001). Our primary prediction was an interaction between reliability and test trial type, with higher levels of reliability leading to worse performance on Switch trials (i.e., less memory allocated to alternative word-object links). To explore this prediction, we performed four complementary analyses: our primary analysis, which tested the effect of the reliability manipulation, and three secondary analyses, which explored the effects of participants' (a) use of the gaze cue, (b) subjective reliability assessments, and (c) inspection time on exposure trials. 

```{r expt3-sub-plots, out.width = "90%", fig.pos = "tb", fig.cap = "Secondary analyses of test trial performance in Experiment 3. Panel A shows accuracy as a function of the number of exposure trials on which participants chose to use the gaze cue. Panel B shows accuracy as a function of participants' subjective reliability judgments. The horizontal dashed lines represent chance performance, and error bars indicate 95\\% confidence intervals computed by non-parametric bootstrap."}
grid::grid.raster(png::readPNG("figs/expt3_sub_plot.png"))
```

#### Reliability condition analysis


```{r e3 analysis}
df_analysis <- df_expt3 %>% 
  filter(trial_category == "exposure", block == "test", include_good_rt == "include") %>% 
  select(subid, itemNum, inspection_time_exposure = rt) %>% 
  mutate(inspection_time_exposure_sec = inspection_time_exposure / 1000) %>% 
  left_join(filter(df_expt3, trial_category == "test", block == "test", include_good_rt == "include"), 
            by = c("subid", "itemNum")) 
```


```{r glmer condition expt3}
m1_expt3 <- glmer(correct ~ (trialType + reliability)^2 + (trialType | subid), 
                  nAGQ = 1,
                  family = binomial,
                  data = df_analysis,
                  control = glmerControl(optimizer = "bobyqa"))

beta_trialtype_expt3 <- round(summary(m1_expt3)$coef[2], 2)
beta_trialtype_int_expt3 <- round(summary(m1_expt3)$coef[3], 2)
beta_rel_trialtype_int_expt3 <- round(summary(m1_expt3)$coef[4], 2)
p_rel_trialtype_int_expt3 <- round(summary(m1_expt3)$coef[4,4], 3)
```

To test the effect of reliability, we fit a model predicting accuracy at test using reliability condition and test trial type as predictors. We found a significant main effect of trial type ($\beta = `r beta_trialtype_expt3`$, $p$ < .001), with lower accuracy on Switch trials. We also found the key interaction between reliability condition and trial type ($\beta$ = `r beta_rel_trialtype_int_expt3`, $p$ = `r p_rel_trialtype_int_expt3`), such that when gaze was more reliable, participants performed worse on Switch trials (see Panel A of Figure 4). This interaction suggests that people stored more word-object links as the learning context becomes more ambiguous. However, the interaction between reliability and trial type was not particularly strong, and -- similar to Experiment 1 -- there was variability in performance across conditions (see the 50% reliable condition in Panel A of Figure 4). So to provide additional support for our hypothesis, we conducted three follow-up analyses. 

#### Gaze use analyses

```{r e3 glmer gaze follow on exposure}
m2a_expt3 <- glmer(correct ~ (correct_exposure + trialType + reliability)^2 + 
                     (trialType | subid),
                    control = glmerControl(optimizer = "bobyqa"),
                    nAGQ = 1,
                    family = binomial,
                    data = df_analysis)

# trial type and gf
beta_trialtype_gf_e3_m2a <- round(summary(m2a_expt3)$coef[5], 2)

# gf and reliability
beta_gf_rel_e3_m2a <- round(summary(m2a_expt3)$coef[6], 2)
p_gf_rel_e3_m2a <- round(summary(m2a_expt3)$coef[6,4], 3)

# reliability and trial type
beta_rel_trialtype_e3_m2a <- round(summary(m2a_expt3)$coef[7], 2)
p_rel_trialtype_e3_m2a <- round(summary(m2a_expt3)$coef[7,4], 3)
```

We would only expect to see a strong interaction between reliability and trial type if learners chose to use the gaze cue during exposure trials. To test this hypothesis, we fit two additional models that included two different measures of participants' use of the gaze cue. First, we added accuracy on exposure trials as a predictor in our model. (Recall that correct performance on exposure trials was defined as using the gaze cue.) We found a significant interaction between accuracy on exposure trials and trial type ($\beta = `r beta_trialtype_gf_e3_m2a`$, $p$ < .001) with worse performance on Switch test trials when participants used gaze on exposure trials (see Panel B of Figure 4). We also found an interaction between gaze use and reliability ($\beta = `r beta_gf_rel_e3_m2a`$, $p$ = `r p_gf_rel_e3_m2a`) such that when gaze was more reliable, participants were more likely to use it. The interaction between trial type and reliability became marginally significant in this model ($\beta = `r beta_rel_trialtype_e3_m2a`$, $p$ = `r p_rel_trialtype_e3_m2a`), suggesting that participants' use of the gaze cue was a stronger predictor of memory for alternative word-object links.^[We are grateful to an anonymous reviewer for suggesting this analysis, but we would like to note that it is exploratory.]

```{r glmer total exposure correct expt3}
m2b_expt3 <- glmer(correct ~ total_exposure_correct * trialType + (trialType | subid),
                  control = glmerControl(optimizer = "bobyqa"),
                  nAGQ = 1,
                  family = binomial,
                  data = filter(df_analysis, experiment == "replication"))

beta_totexpo_expt3 <- round(summary(m2b_expt3)$coef[2], 2)
beta_trialtype_expt3 <- round(summary(m2b_expt3)$coef[3], 2)
beta_totexpo_trialtype_int_expt3 <- round(summary(m2b_expt3)$coef[4], 2)
```

```{r glmer original total exposure correct expt3}
m2_expt3_original <- glmer(correct ~ total_exposure_correct * trialType + (trialType | subid),
                           control = glmerControl(optimizer = "bobyqa"),
                           nAGQ = 1,
                           family = binomial,
                           data = filter(df_analysis, experiment == "original"))

beta_totexpo_expt3_original <- round(summary(m2_expt3_original)$coef[2], 2)
beta_trialtype_expt3_original <- round(summary(m2_expt3_original)$coef[3], 2)
beta_totexpo_trialtype_int_expt3_original <- round(summary(m2_expt3_original)$coef[4], 2)
```

We also hypothesized that the reliability manipulation might change how often individual participants chose to use the gaze cue throughout the task. To explore this possibility, we fit a model with the same specifications, but we included a predictor that we created by binning participants based on the number of exposure trials on which they chose to follow gaze (i.e., a gaze following score). We found a significant interaction between how often participants chose to follow gaze on exposure trials and trial type ($\beta = `r beta_totexpo_trialtype_int_expt3`$, $p$ < .001), such that participants who were more likely to use the gaze cue performed worse on Switch trials, but not Same trials (see Panel B of Figure 5). ^[We found this interaction while performing exploratory data analysis on a previous version of this study with an independent sample (N = 250, $\beta = `r beta_totexpo_trialtype_int_expt3_original`$, $p$ < .001). The results reported here are from a follow-up study where testing this interaction was a planned analysis.] Taken together, the two analyses of participants' use of the gaze cue provide converging evidence that when the speaker's gaze was reliable participants were more likely to use the cue, and when they followed gaze, they tended to store less information from the initial naming event.

#### Subjective reliability analysis 

```{r glmer subjective reliability expt 3}
m3_expt3 <- glmer(correct ~ rel_subj * trialType + (trialType | subid),
                  control = glmerControl(optimizer = "bobyqa"), 
                  nAGQ = 1,
                  family = binomial,
                  data = df_analysis)

beta_rel_subj_expt3 <- round(summary(m3_expt3)$coef[2], 2)
beta_trialtype_rel_subj_expt3 <- round(summary(m3_expt3)$coef[3], 2)
beta_rel_subj_trialtype_int_expt3 <- round(summary(m3_expt3)$coef[4], 2)
p_rel_subj_trialtype_int_expt3 <- round(summary(m3_expt3)$coef[4,4], 2)
```

The strong interaction between use of the gaze cue and memory for alternative word-object links suggests that participants' subjective experience of reliability in the experiment mattered. Thus, we fit the same model but substituted subjective reliability for the frequency of gaze use as a predictor of test trial performance. We found a significant interaction between trial type and participants' subjective reliability assessments ($\beta = `r beta_rel_subj_trialtype_int_expt3`$, $p$ = `r p_rel_subj_trialtype_int_expt3`): when participants thought the speaker was more reliable, they performed worse on Switch trials, but not Same trials (see Panel B of Figure 5). 

#### Inspection time analyses

```{r glmer e3 inspection time}
# does inspection time affect test trial performance
m4a_expt3_2 <- glmer(correct ~ (log2(inspection_time_exposure_sec) + 
                                  trialType + 
                                  reliability)^2 + 
                     (trialType | subid),
                  control = glmerControl(optimizer = "bobyqa"), 
                  nAGQ = 0,
                  family = binomial,
                  data = df_analysis)

# get betas and p.vals
m4a_coefs <- broom::tidy(m4a_expt3_2) %>% 
  mutate(estimate = round(estimate, 2),
         p.value = ifelse(round(p.value, 3) == 0, "< .001", round(p.value, 3)))
```


```{r lmer e3 inspection time}
# what predicts inspection time? --> participants use gaze
m4_inspect_expt3 <- lmer(log2(inspection_time_exposure_sec) ~ reliability * 
                           correct_exposure + 
                           (1 | subid), data = df_analysis)

ms_inspect_e3 <- df_analysis %>% 
  group_by(reliability, subid) %>% 
  summarise(m = mean(inspection_time_exposure_sec)) %>% 
  group_by(reliability) %>% 
  multi_boot_standard(column = "m") %>% 
  filter(is.na(reliability) == F) %>% 
  mutate(mean = round(mean, 2))
```

Finally, we were curious about how inspection time on exposure trials affected accuracy at test. So we fit a model using inspection time, trial type, and reliability condition to predict accuracy. We found a main effect of inspection time ($\beta$ = `r m4a_coefs$estimate[2]`, $p$ = `r m4a_coefs$p.value[2]`), with longer inspection times leading to better performance. The interaction between trial type and inspection time was not significant, meaning that increased inspection time had the same, positive effect on both Same and Switch trials. There was a marginally significant interaction between inspection time and reliability condition  ($\beta$ = `r m4a_coefs$estimate[6]`, $p$ = `r m4a_coefs$p.value[6]`) with longer inspection times providing a larger boost to accuracy when the speaker was less reliable.

Next, we explored the factors that influenced inspection time on exposure trials by fitting a model to predict inspection time using reliability and use of the gaze cue as predictors. We found a main effect of using the gaze cue (`r round(summary(m4_inspect_expt3)$coef[3], 2)`, $p$ < .001) with use of the gaze cue leading to shorter inspection times. The main effect of reliability condition and the interaction between reliability and use of gaze were not significant. These analyses provide evidence that use of the gaze cue was the primary factor affecting how long participants inspected the objects on exposure trials.

Together, these four analyses show that when the speaker's gaze was more reliable, participants were more likely to: (a) use the gaze cue, (b) rate the speaker as more reliable, and (c) store fewer word-object links, showing behavior more consistent with single hypothesis tracking. These findings support and extend the results of Experiments 1 and 2 in several important ways. First, participants' performance on Same trials was again relatively unaffected by changes in performance on Switch trials. The selective effect of gaze on Switch trials provides converging evidence that the limitations on Same trials may be different than those regulating the distribution of attention on Switch trials. Second, learners' use of a referential cue was a stronger predictor of reduced memory for alternative word-object links compared to our reliability manipulation. Although we found a significant effect of reliability on participants' use of the gaze cue, participants' tendency to use the cue remained high. Consider that even in the 0% reliability condition the mean proportion of gaze following was still `r round(min(ms_expt3$accuracy),2)`. It is reasonable that participants would continue to use the gaze cue in our experiment since it was the only cue available and participants did not have a strong reason to think that the speaker would be deceptive. 

The critical contribution of Experiment 3 is to show that learners respond to a graded manipulation of referential uncertainty, with the amount of information stored from the initial exposure tracking with the reliability of the cue. This graded accuracy performance shows that learners stored alternative word-object links with different levels of fidelity depending on the amount of referential uncertainty present during learning. 

Across Experiments 1-3, learners tended to store fewer word-object links in unambiguous learning contexts when a clear referential cue was present. However, in all three experiments, participants' responses on exposure trials controlled the length of the trial, which meant that when participants used the gaze cue, they also spent less time visually inspecting the objects. Thus, we do not know whether there is an independent effect of referential cues on learners' underlying representations, or if the effects found in Experiments 1-3 are entirely mediated by a reduction in inspection time. In Experiment 4, we addressed this possibility by removing participants' control over the length of exposure trials, which made the inspection times on exposure trials equivalent across the Gaze and No-Gaze conditions. 

# Experiment 4

In Experiment 4, we asked whether a reduction in visual inspection time in the gaze condition could completely explain the effect of social cues on learners' reduced memory for alternative word-object links. To answer this question, we modified our paradigm and made the length of exposure trials equivalent across the Gaze and No-Gaze conditions. In this version of the task, participants saw the objects for a fixed amount of time regardless of whether gaze was present. We also included two different exposure trial lengths in order to test whether gaze would have a differential effect at shorter vs. longer inspection times. If the presence of gaze reduces learners' memory for multiple word-object links, then this provides evidence that referential cues affected the underlying representations over and above a reduction in inspection time.

```{r expt4-plot, out.width = "50%", fig.cap = "Experiment 4 results. Accuracy on test trials in Experiment 4 collapsed across the Long and Short inspection time conditions. The dashed line represents chance performance. Color and line type indicate whether there was gaze present on exposure trials. Error bars indicate 95\\% confidence intervals computed by non-parametric bootstrap. "}
grid::grid.raster(png::readPNG("figs/expt4_collapsed.png"))
```

## Method

### Participants

```{r e4 nsubs}
nsubs_expt4 <- df_expt4 %>% 
  group_by(interval, inspection_cond) %>%
  summarise(n_subs = n_distinct(subid))
```

```{r e4 filter}
df_expt4_filtered <- df_expt4 %>% 
  filter(trial_category == "test", 
         answer_type_exposure == "participant_response",
         correct_exposure == T | gaze_trial == "No-Gaze",
         mean_acc_exp > 0.25, include_good_rt == "include")
```

```{r exp4 ss removed}
df_n_expt4 <- df_expt4 %>%
  group_by(interval, inspection_cond) %>%
  summarise(n_subs = n_distinct(subid))

df_n_expt4_filt <- df_expt4_filtered %>%
  group_by(interval, inspection_cond) %>%
  summarise(n_subs_filt = n_distinct(subid)) %>%
  select(n_subs_filt)

nsubs_expt4_filt <- cbind(df_n_expt4, df_n_expt4_filt)

# get number of excluded
n_hits <- 400
expt4_nsubs_excluded = n_hits - sum(nsubs_expt4_filt$n_subs_filt)
```

Participant recruitment and inclusion/exclusion criteria were identical to those of Experiments 1, 2, and 3. 100 HITs were posted for each condition (1 Referent X 2 Intervals X 2 Inspection Time conditions) for total of `r n_hits` paid HITs (`r expt4_nsubs_excluded` HITs excluded). 

### Stimuli

Audio, picture, and video stimuli were identical to Experiments 2 and 3. Since inspection times were fixed across conditions, we wanted to ensure that participants were aware of the time remaining on each exposure trial. So we included a circular countdown timer located above the center video. The timer remained on the screen during test trials but did not count down since participants could take as much time as they wanted to respond on test trials.

### Design and Procedure

Procedures were identical to those of Experiment 1-3. The design was identical to that of Experiment 2 and consisted of 32 trials split into 2 blocks of 16 trials. Each block consisted of 8 exposure trials and 8 test trials (4 Same trials and 4 Switch trials) and contained only Gaze or No-Gaze exposure trials. The order of block was counterbalanced across participants. 

The major design change was to make the length of exposure trials equivalent across the Gaze and No-Gaze conditions. We randomly assigned participants to one of two inspection time conditions: Short (6 seconds) or Long (9 seconds). These times were selected based on participants' self-paced inspection times in the Gaze and No-Gaze conditions in Experiment 2. After pilot testing, we added three seconds to each condition to ensure that participants had enough time to respond before the experiment advanced. If participants did not respond in the allotted time, an error message appeared informing participants that time had run out and encouraged them to respond within the time window on subsequent trials. 

## Results and Discussion

```{r e4 timed out}
e4_timed_out_df <- df_expt4 %>% 
  filter(trial_category == "exposure") %>% 
  group_by(answer_type_exposure) %>% 
  summarise(prop = round(n()/nrow(.), 2))
```

We did not see strong evidence of an effect of the different inspection times. Thus, all of the results reported here collapse across the short and long inspection time conditions. For all analyses, we removed the trials on which participants did not respond within the fixed inspection time on exposure trials (`r e4_timed_out_df$prop[2]`% of trials).

### Exposure Trials

```{r e4 chance exposure}
e4.chance.tests.exposure <- test.chance(data = filter(df_expt4, 
                                                      answer_type_exposure == "participant_response",
                                                      trial_category == "exposure", 
                                                      include_good_rt == "include", 
                                                      gaze_trial == "Gaze"),
                                        groups = c("intervalNum"),
                                        formula = as.formula("correct_exposure ~ 1")) %>%
  mutate(stars = getstars(ps))
```

```{r e4 gf means}
ms_expo_e4 <- df_expt4 %>% 
  filter(trial_category == "exposure",
         answer_type_exposure == "participant_response",
         gaze_trial == "Gaze",
         mean_acc_exp > 0.25,
         include_good_rt == "include") %>% 
  group_by(subid, intervalNum) %>% 
  summarise(m_ss = mean(correct_exposure)) %>% 
  group_by(intervalNum) %>% 
  summarise(m = round(mean(m_ss), 2))
```

Participants' responses on exposure trials differed from those expected by chance (smallest $\beta$ = `r round(sort(e4.chance.tests.exposure$betas)[1], 2)`, z = `r round(sort(e4.chance.tests.exposure$zs)[1], 2)`, $p$ < .001), suggesting that gaze was again effective in directing participants' attention. Similar to Experiment 2, participants were quite likely to use the gaze cue when it was a video of an actress ($M_{0-interval}$ = `r ms_expo_e4$m[1]`, $M_{3-interval}$ = `r ms_expo_e4$m[2]`).

### Test Trials

```{r e4 test chance}
e4.chance.tests <- test.chance(data = df_expt4_filtered,
                               groups = c("trialType","intervalNum", "gaze_trial"),
                               formula = as.formula("correct ~ 1")) %>%
  mutate(stars = getstars(ps))
```

```{r e4 2way glmer}
m1_acc_expt4 <- glmer(correct ~ (trialType + gaze_trial +
                                   log2(intervalNum + 1))^2 +
                        (trialType | subid), 
                      nAGQ=0,
                      glmerControl(optimizer = "bobyqa"),
                      family=binomial,
                      data = df_expt4_filtered)

m1_e4_coefs <- broom::tidy(m1_acc_expt4) %>% 
  mutate_at(.cols = c("estimate", "std.error", "statistic", "p.value"), 
            .funs = round, digits = 3) %>% 
  mutate(estimate = round(estimate, 2))
```

```{r e4 3way glmer}
# note: we might not need this if we present the two-way model
m1_acc_expt4_3way <- glmer(correct ~ (trialType + gaze_trial + 
                                   log2(intervalNum+1) +
                                     inspection_cond)^3 +
                        (trialType | subid), 
                      nAGQ=0,
                      glmerControl(optimizer = "bobyqa"),
                      family=binomial,
                      data = df_expt4_filtered)

m1_e4_3way_coefs <- broom::tidy(m1_acc_expt4_3way) %>% 
  mutate_at(.cols = c("estimate", "std.error", "statistic", "p.value"), 
            .funs = round, digits = 3) %>% 
  mutate(estimate = round(estimate, 2))
```

```{r e4 3way glmer interaction with e2}
e2_df_to_merge <- df_expt2 %>% 
  filter(trial_category == "test", 
         correct_exposure == T | condition_trial == "no-social",
         mean_acc_exp > 0.25, include_good_rt == "include") %>% 
  select(subid, condition_trial, trialType, intervalNum, correct) %>% 
  rename(gaze_trial = condition_trial) %>% 
  mutate(subid = subid + 1000, 
         gaze_trial = ifelse(gaze_trial == "social", "Gaze", "No-Gaze"),
         experiment = "participant_controlled_it")

e4_df_to_merge <- df_expt4_filtered %>% 
  select(subid, gaze_trial, trialType, intervalNum, correct) %>% 
  mutate(experiment = "fixed_it")

# now merge, so we can model these data
e2_e4_final_df <- bind_rows(e2_df_to_merge, e4_df_to_merge)

# is the gaze effect different in the fixed inspection time experiment?
m1_acc_e2_e4 <- glmer(correct ~ (trialType + gaze_trial + experiment)^3 +
                        (trialType | subid), 
                      nAGQ=0,
                      glmerControl(optimizer = "bobyqa"),
                      family=binomial,
                      data = e2_e4_final_df)
```

```{r e4 lsmeans simple effects}
# test pairwise contrasts using lsmeans package
lsm_output <- summary(lsmeans::lsmeans(m1_acc_expt4, 
                               pairwise~gaze_trial|trialType, 
                               adjust="bon"))

same_diff_beta_e4 <- round(lsm_output$contrasts$estimate[1], 2)
switch_diff_beta_e4 <- round(lsm_output$contrasts$estimate[2], 2)
```

Figure 6 shows performance on test trials in Experiment 4. In the majority of conditions, participants selected the correct referent at rates greater than chance (smallest $\beta$ = `r round(sort(e4.chance.tests$betas)[2], 2)`, z = `r round(sort(e4.chance.tests$zs)[2], 2)`, $p$ < .05). However, participants' responses were only marginally different from chance on Switch trials after exposure trials with gaze in the 3-interval condition ($\beta$ = `r round(sort(e4.chance.tests$betas)[1], 2)`, $p$ = `r round(sort(e4.chance.tests$ps, decreasing = T)[1], digits = 2)`). 

We replicate the key finding from Experiments 1-3: after seeing exposure trials with gaze, participants were less accurate on Switch trials ($\beta$ = `r m1_e4_coefs$estimate[5]`, $p$ < .001). Since inspection times were fixed across the Gaze and No-Gaze conditions, this finding provides evidence that the presence of a referential cue did more than just reduce the amount of time participants' spent inspecting the potential word-object links. In contrast to Experiments 1-3, visual inspection of Figure 6 suggested that the referential cue provided a boost to accuracy on Same trials. To assess the simple effects of gaze on trial type, we computed pairwise contrasts using the *lsmeans* package in R with a Bonferroni correction for multiple comparisons [@lenth2016lsmeans]. Accuracy was higher for Same trials in the Gaze condition ($\beta$ = `r same_diff_beta_e4`, $p$ < .001), but lower for Switch trials ($\beta$ = `r switch_diff_beta_e4`, $p$ < .001). The boost in accuracy on Same trials differs from Experiments 1-3 and suggests that making inspection times equivalent across conditions allowed the social cue to affect the strength of learners' memory for their candidate hypothesis.

The results of Experiment 4 help to clarify the effect of gaze on memory in our task, providing evidence that the presence of a referential cue did more than just reduce participants' visual inspection time. Instead, gaze reduced memory for alternative word-object links even when people had the same opportunity to visually inspect and encode them. We also found evidence of a boost for learners' memory of their candidate hypothesis in the gaze condition, an effect that we did not see in Experiments 1-3. One explanation for this difference is that in Experiment 4, since participants' use of gaze was independent of the length of exposure trials, inspection times in the gaze condition were longer compared to those in Experiments 1-3. Thus, it could be that the combination of a gaze cue with the opportunity to continue attending to the gaze target led to a boost in performance on Same trials relative to trials without gaze.

# General Discussion

Tracking cross-situational word-object statistics allows word learning to proceed despite the presence of individually ambiguous naming events. But models of cross-situational learning disagree about how much information is actually stored in memory, and the input to statistical learning mechanisms can vary along a continuum of referential uncertainty from unambiguous naming instances to highly ambiguous situations. In the current line of work, we explore the hypothesis that these two factors are fundamentally linked to one another and to the social context in which word learning occurs. Specifically, we ask how cross-situational learning operates over social input that varies the amount of ambiguity in the learning context. 

Our results suggest that the representations underlying cross-situational learning are quite flexible. In the absence of a referential cue to word meaning, learners tended to store more alternative word-object links. In contrast, when gaze was present learners stored less information, showing behavior consistent with tracking a single hypothesis (Experiments 1 and 2). Learners were also sensitive to a parametric manipulation of the strength of the referential cue, showing a graded increase in the tendency to use the cue as reliability increased, which in turn resulted in a graded decrease in memory for alternative word-object links (Experiment 3). Finally, learners stored less information in the presence of gaze even when they spent the same amount of time visually inspecting the objects during learning (Experiment 4). 

In Experiments 1-3, reduced memory for alternative hypotheses did not result in a strong boost to memory for learners' candidate hypothesis. This pattern of data suggests that the presence of a referential cue selectively affected one component of the underlying representation: the number of alternative word-object links, and not learners' candidate hypothesis. However, when the length of exposure trials was equivalent across the gaze and no-gaze conditions in Experiment 4, learners showed stronger memory for their initial hypothesis when gaze was present, suggesting that the confound with inspection time might have masked this difference in the previous experiments. This finding in turn suggests that the relationship between referential cues and the strength of learners' candidate hypothesis is modulated by how the cue interacts with attention: When coupled with the opportunity for sustained attention, gaze provided a boost to memory.

## Relationship to previous work

Why would a decrease in memory for alternatives fail to increase the strength of learners' memory for their candidate hypothesis in Experiments 1-3? One possibility is that participants did not shift their cognitive resources from the set of alternatives to their single hypothesis, but instead chose to use the gaze information to reduce inspection time, thus conserving their resources for future use. `r knitcitations::citet(bib[["griffiths2015rational"]])` formalize this behavior by pushing the rationality of computational-level models down to the psychological process level. In their framework, cognitive systems are thought to be adaptive in that they optimize the use of their limited resources, taking the cost of computation (e.g., the opportunity cost of time or mental energy) into account. For example, `r knitcitations::citet(bib[["vul2014"]])` showed that as time pressure increased in a decision-making task, participants were more likely to show behavior consistent with a less cognitively challenging strategy of matching, rather than with the globally optimal strategy. In the current work, we found that learners showed evidence of altering how they allocated cognitive resources based on the amount of referential uncertainty present during learning, spending less time inspecting alternative word-object links and reducing the number of links stored in memory when uncertainty was low.

Our results fit well with recent experimental work that investigates how attention and memory can constrain infants' statistical word learning. For example, `r knitcitations::citet(bib[["smith2013visual"]])` used a modified cross-situational learning task to show that only infants who disengaged from a novel object to look at both potential referents were able to learn the correct word–object mappings. Moreover, `r knitcitations::citet(bib[["vlach2013memory"]])` showed that 16-month-olds were only able to learn from adjacent cross-situational co-occurrence statistics, and unable to learn from co-occurrences that were separated in time. Both of these findings make the important point that only the information that comes into contact with the learning system can be used for cross-situational word learning, and this information is directly influenced by the attention and memory constraints of the learner. These results also add to a large literature showing the importance of social information for word learning [@bloom2002children; @clark2009first] and to recent work exploring the interaction between statistical learning mechansims and other types of information [@yu2007unified; @frank2009using; @koehne2014interplay]. Our findings suggest that referential cues affect statistical learning by modulating the amount of information that people store in the underlying representations that support learning over time.

Is gaze a privileged cue, or could other, less-social cues (e.g., an arrow) also affect the representations underlying cross-situational learning? On the one hand, previous research has shown that gaze cues lead to more reflexive attentional responses compared to arrows [@friesen2004attentional], that gaze-triggered attention results in better learning compared to salience-triggered attention [@wu2010no], and that even toddlers readily use gaze to infer novel word meanings [@baldwin1993infants]. Thus, it could be that gaze is an especially effective cue for constraining word learning since it communicates a speaker's referential intent and is a particularly good way to guide attention. On the other hand, the generative process of the cue -- whether it is more or less social in nature -- might be less important; instead, the critical factor might be whether the cue effectively reduces uncertainty in the naming event. Under this account, gaze is placed amongst a set of many cues that could produce similar effects as those reported here. Future work could explore a wider range of cues to see if they modulate the representations underlying cross-situational learning in a similar way. 

How should we characterize the effect of gaze on attention and memory in our task? One possibility is that the referential cue acts as a filter, only allowing likely referents to contact statistical learning mechanisms [@yu2007unified]. This 'filtering account' separates the effect of social cues from the underlying computation that aggregates cross-situational information. Another possibility is that referential cues provide evidence about a speaker's communicative intent [@frank2009using]. In this model, the learner is reasoning about the speaker and word meanings simultaneously, which places inferences based on social information as part of the underlying computation. A third possibility is that participants thought of the referential cue as pedagogical. In this context, learners assume that the speaker will choose an action that is most likely to increase the learner's belief in the true state of the world [@shafto2012learning], making it unnecessary to allocate resources to alternative hypotheses. Experiments show that children spend less time exploring an object and are less likely to discover alternative object-functions if a single function is demonstrated in a pedagogical context [@bonawitz2011double]. However, because the results from the current study cannot distinguish between these explanations, these questions remain topics for future studies specifically designed to tease apart these possibilities. 

## Limitations

There are several limitations to the current study that are worth noting. First, the social context that we used was relatively impoverished. Although we moved beyond a simple manipulation of the presence or absence of social information in Experiment 3, we nevertheless isolated just a single cue to reference, gaze. But real-world learning contexts are much more complex, providing learners access to multiple cues such as gaze, pointing, and previous discourse. In fact, `r knitcitations::citet(bib[["frank2013social"]])` analyzed a corpus of parent-child interactions and concluded that learners would do better to aggregate noisy social information from multiple cues, rather than monitor a single cue since no single cue was a consistent predictor of reference. In our data, we did see a more reliable effect of referential cues when we used video of an actress, which included both gaze and head turn as opposed to the static, schematic stimuli, which only included gaze. It is still an open and interesting question as to how our results would generalize to learning environments that contain a rich combination of social cues.

Second, we do not yet know how variations in referential uncertainty during learning would affect the representations of young word learners, the age at which cross-situational word learning might be particularly important. Recent research using a similar paradigm as our own did not find evidence that 2- or 3-year-olds stored multiple word-object links; instead, children only retained a single candidate hypothesis [@woodard2016two]. However, performance limitations on children's developing attention and memory systems [@ross2003development; @colombo2001development] could make success on these explicit response tasks more difficult. Moreover, our work suggests that different levels of referential uncertainty in naturalistic learning contexts [@medina2011words; @yurovsky2014algorithmic] might evoke different strategies for information storage, with learners storing more information as ambiguity increases. Thus, we think that it will be important to test a variety of outcome measures and learning contexts to see if younger learners show evidence of storing multiple word meanings during learning. 

Previous work with infants has shown that their attention is often stimulus-driven and sticky [@oakes2011infant], suggesting that very young word learners might not effectively explore the visual scene in order to extract the necessary statistics for storing multiple alternatives. It could be that referential cues play an even more important role for young learners by filtering the input to cross-situational word learning mechanisms and guiding children to the relevant statistics in the input. In fact, recent work has shown that the precise timing of features such as increased parent attention and gesturing towards a named object and away from non-target objects were strong predictors of referential clarity in a naming event [@trueswell2016perceiving]. It could be that the statistics available in these particularly unambiguous naming events are the most useful for cross-situational learning.

Finally, the current experiments used a restricted cross-situational word learning scenario, which differs from real-world language learning contexts in several important ways. One, we only tested a single exposure for each novel word-object pairing; whereas, real-world naming events are best characterized by discourse where an object is likely to be named repeatedly in a short amount of time [@frank2013social; @rohde2014markers]. Two, the restricted visual world of 2-8 objects on a screen combined with the forced-choice response format may have biased people to assume that all words in the task must have referred to one of the objects. But, in actual language use, people can refer to things that are not physically co-present [e.g., @gleitman1990structural], creating a scenario where learners would not benefit from storing additional word-object links in the absence of clear referential cues. Finally, we presented novel words in isolation, removing any sentential cues to word meaning (e.g., verb-argument relations). In fact, previous work with adults has shown that cross-situational learning mechanisms only operate in contexts where sentence-level constraints do not completely disambiguate meaning [@koehne2014interplay]. Thus, we need more evidence to understand how the representations underlying cross-situational learning change in response to referential uncertainty at different timescales and in richer language contexts that more accurately reflect real-world learning environments.

## Conclusions

Word learning proceeds despite the potential for high levels of referential uncertainty and despite learners’ limited cognitive resources. Our work shows that cross-situational learners flexibly respond to the amount of ambiguity in the input, and as referential uncertainty increases, learners tended to store more word-object links. Overall, these results bring together aspects of social and statistical accounts of word learning to increase our understanding of how statistical learning mechansims operate over fundamentally social input.

\newpage

# Acknowledgements

We are grateful to Rose Schneider for helping record stimuli and to the members of the Language and Cognition Lab for their feedback on this project. This work was supported by a National Science Foundation Graduate Research Fellowship to KM, an NIH NRSA Postdoctoral Fellowship to DY, and a John Merck Scholars Fellowship to M.C.F. 

\newpage

# References 

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent

```{r}
# Below we list the in-text citations that were aded to the paper using the knitcitations package.
# Otherwise, the in-text citations would not appear in the reference list.
```
---
nocite: | 
  @smith2014unrealized
  @koenig2004trust
  @smith2013visual
  @smith2008infants
  @yurovsky2014algorithmic
  @medina2011words
  @yurovsky2013statistical
  @cartmill2013quality
  @frank2013social
  @yoshida2012exclusion
  @vlach2013memory
  @vul2014
  @griffiths2015rational
  @kanwisher1997locus
  @yu2012embodied
  @gillette1999human
...